{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c87419bb-ca21-4ce4-b619-d99d14f3fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import mlflow \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "from optbinning import OptimalBinning\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from scipy.stats import f_oneway\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc7c9d-26d2-46fb-b4cf-d5d3039ea0a5",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9359744c-de0a-4aca-885e-f861f70944d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_return(num):\n",
    "    mu = 0\n",
    "    sigma = 2\n",
    "    # return np.random.beta(a, b, size=num)\n",
    "    # normal = np.random.normal(0, 1, num)\n",
    "    # normal = norm.cdf(normal)\n",
    "    # return normal\n",
    "    return np.random.normal(mu, sigma, num)\n",
    "\n",
    "random.seed(10)\n",
    "def generate_data(num):\n",
    "    df = pd.DataFrame({'party_id':[i for i in range(num)],\n",
    "                      'risk_probability': [random.uniform(a = 0, b = 1) for i in range(num)]\n",
    "                      })\n",
    "    \n",
    "    df['high_risk_flag'] = [0 if i<0.50 else 1 for i in df['risk_probability']]\n",
    "    ##################### BILL  ##############################\n",
    "    df['BILL_OUTSTANDING_M1_avg'] = 5000*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_AMT_M1_avg'] = 4000*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_RENTALS_M1_avg'] = 700*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_RENTALS_M2_avg'] = 1700*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_RENTALS_M3_avg'] = 1600*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_RENTALS_M4_avg'] = 400*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_RENTALS_M5_avg'] = 350*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_RENTALS_M6_avg'] = 400*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_USAGE_CHRGS_M1_avg'] = 0.02*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_USAGE_CHRGS_M2_avg'] = 0.2*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_USAGE_CHRGS_M3_avg'] = 0.2*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_USAGE_CHRGS_M4_avg'] = 0.15*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_USAGE_CHRGS_M5_avg'] = 1300*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_USAGE_CHRGS_M6_avg'] = 1400*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_OUTSTANDING_M1_sum'] = 5000*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_OUTSTANDING_M3_avg'] = 3500*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_OUTSTANDING_M3_sum'] = 7000*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_OUTSTANDING_M6_avg'] = 3000*(df['risk_probability'] + prob_return(num) )\n",
    "    df['BILL_OUTSTANDING_M6_sum'] = 4000*(df['risk_probability'] + prob_return(num) )\n",
    "    ########### ACTV ##############\n",
    "    df['ACTV_30_DYS_avg'] = df['risk_probability'] + prob_return(num)\n",
    "    df['ACTV_60_DYS_avg'] = df['risk_probability'] + prob_return(num)\n",
    "    df['ACTV_90_DYS_avg'] = df['risk_probability'] + prob_return(num)\n",
    "    df['ACTV_30_DYS_norm'] = 50*(df['risk_probability'] + prob_return(num))\n",
    "    df['ACTV_60_DYS_norm'] = 100*(df['risk_probability'] + prob_return(num))\n",
    "    df['ACTV_90_DYS_norm'] = 25*(df['risk_probability'] + prob_return(num))\n",
    "    \n",
    "    ########### ARPU #################\n",
    "    df['ARPU_12_avg'] = 350*(df['risk_probability'] + prob_return(num))\n",
    "    df['ARPU_1_avg'] = 3000*(df['risk_probability'] + prob_return(num))\n",
    "    df['ARPU_3_avg'] = 150*(df['risk_probability'] + prob_return(num))\n",
    "    df['ARPU_6_avg'] = 2000*(df['risk_probability'] + prob_return(num))\n",
    "    df['ARPU_12_sum'] = 200*(df['risk_probability'] + prob_return(num))\n",
    "    df['ARPU_1_sum'] = 325*(df['risk_probability'] + prob_return(num))\n",
    "    df['ARPU_3_sum'] = 150*(df['risk_probability'] + prob_return(num))\n",
    "    df['ARPU_6_sum'] = 3500*(df['risk_probability'] + prob_return(num))\n",
    "    df\n",
    "    ############# FLG ##############\n",
    "    df['BAR_FLG_M6_N_norm'] = 97*(df['risk_probability'] + prob_return(num))\n",
    "    df['BAR_FLG_M6_Y_norm'] = 52*(df['risk_probability'] + prob_return(num))\n",
    "    df['BAR_FLG_N_norm'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['BAR_FLG_Y_norm'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['CC_CARD_FLG_count']  = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['CST_CAT_FLG_EP_norm'] = 73*(df['risk_probability'] + prob_return(num))\n",
    "    df['CST_CAT_FLG_NP_norm'] = 87*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_N_FLG_count'] = 0.5*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_N_FLG_norm'] = 86*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_Other_FLG_count'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_Other_FLG_norm'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_Y_FLG_count'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_Y_FLG_norm'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_N_FLG_count'] = 0.5*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_N_FLG_norm'] = 86*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_Other_FLG_count'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_Other_FLG_norm'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_Y_FLG_count'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_Y_FLG_norm'] = 1*(df['risk_probability'] + prob_return(num))\n",
    "    \n",
    "    \n",
    "    ############## VO #############\n",
    "    df['VO_MOB_MOU_FREE_M1'] = 13500*(df['risk_probability'] + prob_return(num))\n",
    "    df['VO_MOB_MOU_OFFNT_M1'] = 3156*(df['risk_probability'] + prob_return(num))\n",
    "    df['VO_MOB_MOU_ONNT_M1'] = 15500*(df['risk_probability'] + prob_return(num))\n",
    "    \n",
    "    ############ PORT #############\n",
    "    df['MNP_PORT_IN_0_count'] = prob_return(num)\n",
    "    df['MNP_PORT_OUT_0_count'] = prob_return(num)\n",
    "    df['MNP_PORT_IN_1_count'] = prob_return(num)\n",
    "    df['MNP_PORT_OUT_1_count'] =prob_return(num)\n",
    "    df['MNP_PORT_IN_0_norm'] = prob_return(num)\n",
    "    df['MNP_PORT_OUT_0_norm'] =prob_return(num)\n",
    "    df['MNP_PORT_IN_1_norm'] = prob_return(num)\n",
    "    df['MNP_PORT_OUT_1_norm'] =prob_return(num)\n",
    "    \n",
    "    df['PORTIN_N_FLG_count']=0.5*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_N_FLG_norm']=86*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTIN_Y_FLG_count'] =prob_return(num)\n",
    "    df['PORTIN_Y_FLG_norm']=prob_return(num)\n",
    "    df['PORTIN_Other_FLG_count']=prob_return(num)\n",
    "    df['PORTIN_Other_FLG_norm']=prob_return(num)\n",
    "    \n",
    "    \n",
    "    df['PORTOUT_N_FLG_count']=0.5*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_N_FLG_norm']=86*(df['risk_probability'] + prob_return(num))\n",
    "    df['PORTOUT_Y_FLG_count'] =prob_return(num)\n",
    "    df['PORTOUT_Y_FLG_norm']=prob_return(num)\n",
    "    df['PORTOUT_Other_FLG_count']=prob_return(num)\n",
    "    df['PORTOUT_Other_FLG_norm']=prob_return(num)\n",
    "    \n",
    "    ####### CST ################\n",
    "    df['CST_BAD_DEBT_AMT'] = 6200*(df['risk_probability'] + prob_return(num))\n",
    "    df['CST_BAD_DEBT_RATIO'] = 700*(df['risk_probability'] + prob_return(num))\n",
    "    df['CST_BILL_AMT_M1']  = 6500*(df['risk_probability'] + prob_return(num))\n",
    "    \n",
    "    \n",
    "    df['OS_TO_PMNT_RATIO_avg']  = 3000*(df['risk_probability'] + prob_return(num))\n",
    "    df['OS_TO_PMNT_RATIO_norm']  = 317000*(df['risk_probability'] + prob_return(num))\n",
    "    df['PMNT_AMT_M1_avg']  = 1500*(df['risk_probability'] + prob_return(num))\n",
    "    df['PMNT_AMT_M1_sum']  = 3500*(df['risk_probability'] + prob_return(num))\n",
    "    df['PMNT_CHNL_CNT_M1_max']  = 1.5*(df['risk_probability'] + prob_return(num))\n",
    "    df['PMNT_CNT_M1_avg']  = 1.03*(df['risk_probability'] + prob_return(num))\n",
    "    df['PMNT_CNT_M1_max']  = 1.5*(df['risk_probability'] + prob_return(num))\n",
    "    df['TOP_CHNL_TOT_PMNT_RAT_M1_avg']  = 1.53*(df['risk_probability'] + prob_return(num))\n",
    "    \n",
    "    df['FNP_portin_count'] = abs(10*np.random.normal(size = num)).astype(int)\n",
    "    df['FNP_portin_norm'] = abs(10*np.random.normal(size = num)).astype(int)\n",
    "    df['FNP_portout_count'] = abs(10*np.random.normal(size = num)).astype(int)\n",
    "    df['FNP_portout_norm'] = abs(10*np.random.normal(size = num)).astype(int)\n",
    "    df = create_categorical(df, ['GOV-SALES-VIP-AUH','Growth','Unknown','Missing'], 'CST_CORP_SEG_mode')\n",
    "    df = create_categorical(df, ['1*STAR','4*STAR','2*STAR','PRSTG_MGMT','PRESTIGE','3*STAR','NEW CUST','5*STAR','Unknown','DORMANT','Missing'], 'CST_PFT_ARPU_BAND_mode')\n",
    "    df = create_categorical(df, ['None', 'EMIRATI BRONZE','UNIDENTIFIED', 'PRESTIGE BY MGMT', 'BRONZE','VVVIP', 'GOLD_SPECIAL_NEEDS', 'EMIRATI GOLD', 'PRESTIGE PLATINUM', 'PRESTIGE Ana Emirati', 'EMIRATI SILVER','EMIRATI WELCOME', 'PRESTIGE SOLITAIRE', 'SILVER', 'GOLD', 'WELCOME','YOUTH', 'Missing'] , 'CST_PFT_ARPU_SEG_mode')\n",
    "    df = create_categorical(df, ['CONSUMER'], 'CST_SEG_TP_mode')\n",
    "    df = create_categorical(df, ['Diamond','Unknown','Platinum','Gold'], 'CST_VAL_SEG_mode')\n",
    "    df = create_categorical(df, ['Unknown', 'BankAdvice', 'Cheque', 'Card', 'Mix', 'Wallet', 'Cash', 'Loyalty', 'None'],'PMNT_TOP_CHNL_M1_mode') \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_categorical(df, pop_vals, col_name):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(pop_vals)\n",
    "    le.transform(pop_vals)\n",
    "    noisy_probs = (df['risk_probability'] + np.random.beta(11,11,size = len(df)) )\n",
    "    noisy_probs = [0.99 if i>1 else i for i in noisy_probs]\n",
    "    values = [int(i*len(pop_vals)) for i in noisy_probs]\n",
    "    transformed = le.inverse_transform(values)\n",
    "    df[col_name]  = transformed\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae285ea-9a56-4bd9-a767-e45afde734fb",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4b519fdb-9846-48ef-bda2-0fccd6b23162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SeperateDataTypes(df):\n",
    "    numeric_cols = df.drop(columns = ['party_id','risk_probability','high_risk_flag']).select_dtypes(include = ['number']).columns.tolist()\n",
    "    obj_cols = df.drop(columns = ['party_id','risk_probability','high_risk_flag']).select_dtypes(exclude = ['number']).columns.tolist()\n",
    "    cols = {'numeric':numeric_cols, 'obj':obj_cols}\n",
    "    return cols\n",
    "\n",
    "def WeightOfEvidence(df, target):\n",
    "    seperated_cols = SeperateDataTypes(df)\n",
    "    temp_df = df.copy()\n",
    "    numeric = pd.DataFrame()\n",
    "    for variable in seperated_cols['numeric']:\n",
    "        numeric[variable] = CreateWoEColumn(temp_df, variable, target)\n",
    "    for variable in seperated_cols['obj']:\n",
    "        temp_df[variable] = CreateBins(temp_df, variable, target)\n",
    "        numeric[variable] = CreateWoEColumn(temp_df, variable, target)\n",
    "    numeric[target] = df[target]\n",
    "    \n",
    "    return numeric\n",
    "\n",
    "def CreateWoEColumn(df, variable, target):\n",
    "    x = df[variable].values\n",
    "    y = df[target].values\n",
    "    optb = OptimalBinning(name=variable, dtype='numerical', solver=\"cp\")\n",
    "    optb.fit(x, y)\n",
    "    return optb.transform(x, metric = 'woe')\n",
    "\n",
    "def CreateBins(df, variable, target):\n",
    "    grouped_df = df.groupby(variable).agg({target : ['mean','std']}).round(2).reset_index()\n",
    "    cols = [variable,'mean', 'std']\n",
    "    grouped_df.columns = cols\n",
    "    grouped_df.fillna(0, inplace = True)\n",
    "    grouped_df['map'] = (grouped_df['mean'].astype(str) + '_' + grouped_df['std'].astype(str)).rank(method='dense')\n",
    "    grouped_df[[variable,'map']]\n",
    "    return df[variable].map(grouped_df.set_index(variable)['map'])\n",
    "    \n",
    "def CreateFeatureDF(df, target):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    col_types = SeperateDataTypes(df)\n",
    "    numeric = df[col_types['numeric']]\n",
    "    obj = df[col_types['obj']]\n",
    "    for col in numeric.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        numeric[col] = scaler.fit_transform(numeric[[col]])\n",
    "    for col in obj.columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(obj[col])\n",
    "        obj[col] = le.transform(obj[col])\n",
    "    return pd.concat([df[[target]],numeric, obj], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea379ee-669e-445c-b10c-63038d8a453c",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "26794a11-f5a0-4417-a918-497ef74385d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunFeatureSelection(X_train, y_train, top_n=False):\n",
    "    important_feature_list = []\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    grid = {\n",
    "        'RandomForest':RandomForestClassifier(),\n",
    "        'DecisionTree':DecisionTreeClassifier(),\n",
    "        'XGBoost':XGBClassifier()\n",
    "    }\n",
    "    if top_n:\n",
    "        for model_name in grid.keys():\n",
    "            model = grid[model_name]\n",
    "            model.fit(X_train, y_train)\n",
    "            feature_imp = pd.DataFrame({'var':model.feature_names_in_, 'imp':model.feature_importances_})\n",
    "            feature_imp.sort_values('imp', ascending = False, inplace = True)\n",
    "            important_feature_list.extend(feature_imp.head(top_n)['var'].values)\n",
    "        important_feature_list = np.unique(important_feature_list)\n",
    "        return important_feature_list\n",
    "    else:\n",
    "        for model_name in grid.keys():\n",
    "            model = grid[model_name]\n",
    "            model.fit(X_train, y_train)\n",
    "            feature_imp = pd.DataFrame({'var':model.feature_names_in_, 'imp':model.feature_importances_})\n",
    "            feature_imp.sort_values('imp', ascending = False, inplace = True)\n",
    "            feature_imp['ModelName'] = model_name\n",
    "            feature_imp = feature_imp[feature_imp.imp>0]\n",
    "            feature_importance_df = pd.concat([feature_importance_df, feature_imp])            \n",
    "        return feature_importance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93547a-4c71-4f1f-a30d-d4ff380fc47d",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "144c265c-8328-4170-9b82-a19e0b9666da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModelSearch(X_train,y_train,X_test, y_test, experiment_name, run_prefix):\n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "    mlflow.set_experiment(experiment_name = experiment_name)\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'XGBoost': xgb.XGBClassifier(objective='binary:logistic', random_state=42),\n",
    "        'LGBM': lgb.LGBMClassifier()\n",
    "    }\n",
    "    \n",
    "    # Define parameter grids for each classifier\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.01, 0.1, 1, 10, 100]},\n",
    "        'Decision Tree': {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "        'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]},\n",
    "        'XGBoost': {'max_depth': [3, 6, 9], 'learning_rate': [0.1, 0.01, 0.001]},\n",
    "        'LGBM': {'max_depth': [3, 6, 9], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "    }\n",
    "    \n",
    "    # Perform grid search and evaluate each classifier\n",
    "    for classifier_name, classifier in classifiers.items():\n",
    "        print(f\"Running GridSearchCV for {classifier_name}...\")\n",
    "        \n",
    "        # Perform grid search\n",
    "        with mlflow.start_run(run_name=run_prefix+'_'+classifier_name):\n",
    "            grid_search = GridSearchCV(classifier, param_grid=param_grids[classifier_name], cv=3, scoring='accuracy')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "        \n",
    "            # Get best parameters and best estimator\n",
    "            best_params = grid_search.best_params_\n",
    "            best_estimator = grid_search.best_estimator_\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = best_estimator.predict(X_test)\n",
    "            \n",
    "            # Evaluate model performance\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred)\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            metrics = {\n",
    "                'accuracy':round(accuracy_score(y_test, y_pred)*100,2),\n",
    "                'recall':round(recall_score(y_test, y_pred)*100,2),\n",
    "                'precision':round(precision_score(y_test, y_pred)*100,2),\n",
    "                'f1_score':round(f1_score(y_test, y_pred)*100,2)\n",
    "            }\n",
    "            mlflow.log_param('best_params',best_params)\n",
    "            mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba15274-7b1d-480a-bc83-7c05787991a3",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54b21f-de8c-43cb-9807-ac4040bacc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_data(10000)\n",
    "woe_df = WeightOfEvidence(df,'high_risk_flag')\n",
    "feature_df = CreateFeatureDF(df,'high_risk_flag')\n",
    "experiment_name=f'ModelGridSearch_'+datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df.drop(columns = ['high_risk_flag']), \n",
    "                                                    feature_df.high_risk_flag, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "selected_features = RunFeatureSelection(X_train, y_train, top_n=10)\n",
    "RunModelSearch(X_train[selected_features], y_train, X_test[selected_features], y_test, experiment_name=experiment_name, run_prefix='Scaled')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(woe_df.drop(columns = ['high_risk_flag']), \n",
    "                                                    woe_df.high_risk_flag, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "selected_features = RunFeatureSelection(X_train, y_train, top_n=10)\n",
    "RunModelSearch(X_train[selected_features], y_train, X_test[selected_features], y_test, experiment_name=experiment_name, run_prefix='WoE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d83a2-c6f0-4b46-8331-0164c9f308b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
